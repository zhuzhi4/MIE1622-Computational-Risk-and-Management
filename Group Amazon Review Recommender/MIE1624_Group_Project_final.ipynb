{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MIE1624 Group Project_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqJVPj6LijX2"
      },
      "source": [
        "# MIE 1624 Rating Recommendation System\n",
        "- Group 8\n",
        "\n",
        "\n",
        "We will divide it into three main parts:\n",
        "\n",
        "\n",
        "* 1)Data Cleaning\n",
        "\n",
        "* 2)Model Implementation\n",
        "\n",
        "* 3)Test Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4XrGl0ijX7"
      },
      "source": [
        "import required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2qrvjs5kKzI",
        "outputId": "ece3f27c-63b7-4e21-e727-14870aaba837"
      },
      "source": [
        "# If running on GoogleColab\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.7/dist-packages (2.7.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "u1yKgkAHnCBW",
        "outputId": "eae34f80-4f60-42b3-973e-3e065f4be296"
      },
      "source": [
        "# If running on Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee1a4e98-df55-4cb0-bdcb-0bd59a678379\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee1a4e98-df55-4cb0-bdcb-0bd59a678379\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train (1).csv\n",
            "Saving rating_pairs.csv to rating_pairs.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oELZCpYIijX8",
        "outputId": "b36a7f04-c368-4e18-a2da-591e5376d34d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import re, nltk\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "from whoosh import index, writing, scoring\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "import os, os.path\n",
        "import shutil\n",
        "from collections import Counter\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "#################################################################\n",
        "from html.parser import HTMLParser\n",
        "from pandas import DataFrame\n",
        "#################################################################\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "#################################################################\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "##################################################################\n",
        "import copy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "##################################################################\n",
        "import html \n",
        "import collections \n",
        "from collections import Counter\n",
        "\n",
        "##################################################################\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPpeC_dLijX-"
      },
      "source": [
        "## 1. Data Processing\n",
        "\n",
        "We have two files: train.csv and test.csv. We will use train.csv to build and train a model used for predicting the rating of the amazon product.\n",
        "\n",
        "We can see that there are 11 columns in the train.csv while 10 columns in the test.csv which doesn't have the column called overall. \n",
        "\n",
        "To prepare the features used for prediction. we will select the column summary and review text as our main raw data. Afterwards we will use NLP to process extract the important words using different methods\n",
        "\n",
        "The procedure will follows as below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfpmlBlqijX-"
      },
      "source": [
        "### 1.1 Necessary Functions used for data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foa2CD3HijX-"
      },
      "source": [
        "def remove_nonAlphaNum(tweet):\n",
        "    return re.sub(r'[^\\s\\w]+', '', str(tweet))\n",
        "\n",
        "def removeWhiteSpace(tweet):\n",
        "    tweet = str(tweet)\n",
        "    return tweet.replace('\\r', '').replace('\\n', '')\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    sentence = str(sentence)\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize((sentence)))\n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-hF9fRHijX_"
      },
      "source": [
        "### 1.2 Train Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "r4NzV0qdijX_",
        "outputId": "917bb3e5-53b9-4baa-fbed-54c9eb81e145"
      },
      "source": [
        "# Colab use the first df\n",
        "# anaconda use the second\n",
        "#import io\n",
        "#df = pd.read_csv(io.BytesIO(uploaded['train.csv']))\n",
        "df = pd.read_csv('train (1).csv', error_bad_lines = False)#,engine='python')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>category</th>\n",
              "      <th>price</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewHash</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>08 26, 2014</td>\n",
              "      <td>u92990698</td>\n",
              "      <td>A contemporary jazz and soul performer who's m...</td>\n",
              "      <td>\" CLASSIQUE \"</td>\n",
              "      <td>1409011200</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$8.40</td>\n",
              "      <td>p23649501</td>\n",
              "      <td>3856620</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>05 11, 2003</td>\n",
              "      <td>u36200649</td>\n",
              "      <td>Very good idea to put both the 'pop' and 'orch...</td>\n",
              "      <td>Exceeded my Expectations - This album RAWKS!</td>\n",
              "      <td>1052611200</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$10.98</td>\n",
              "      <td>p58458313</td>\n",
              "      <td>56086781</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>12 5, 2017</td>\n",
              "      <td>u10721702</td>\n",
              "      <td>This is a great collection of Carole King's so...</td>\n",
              "      <td>A Must-have for Carole King Fans</td>\n",
              "      <td>1512432000</td>\n",
              "      <td>Pop</td>\n",
              "      <td>$5.99</td>\n",
              "      <td>p97027626</td>\n",
              "      <td>55852154</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>09 24, 2013</td>\n",
              "      <td>u86003775</td>\n",
              "      <td>The is album is a brilliant piece of Jazz fusi...</td>\n",
              "      <td>A Master piece!</td>\n",
              "      <td>1379980800</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$14.64</td>\n",
              "      <td>p43167086</td>\n",
              "      <td>43228100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>01 14, 2015</td>\n",
              "      <td>u25672859</td>\n",
              "      <td>Waited a LONG time for this DVD to be released...</td>\n",
              "      <td>especially if you like concert videos</td>\n",
              "      <td>1421193600</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$9.92</td>\n",
              "      <td>p94494236</td>\n",
              "      <td>54425467</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall   reviewTime reviewerID  ...     itemID reviewHash  image\n",
              "0      5.0  08 26, 2014  u92990698  ...  p23649501    3856620    NaN\n",
              "1      5.0  05 11, 2003  u36200649  ...  p58458313   56086781    NaN\n",
              "2      5.0   12 5, 2017  u10721702  ...  p97027626   55852154    NaN\n",
              "3      5.0  09 24, 2013  u86003775  ...  p43167086   43228100    NaN\n",
              "4      5.0  01 14, 2015  u25672859  ...  p94494236   54425467    NaN\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trVmVFouijX_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94025184-46de-4451-8773-ddcc188c9932"
      },
      "source": [
        "df_overall = df[[\"overall\"]]\n",
        "df[\"text\"] = df[\"reviewText\"] + df[\"summary\"]      #combine summary and reviewText\n",
        "df[\"text\"] = df[\"text\"].str.lower()                #lowercase\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>category</th>\n",
              "      <th>price</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewHash</th>\n",
              "      <th>image</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>08 26, 2014</td>\n",
              "      <td>u92990698</td>\n",
              "      <td>A contemporary jazz and soul performer who's m...</td>\n",
              "      <td>\" CLASSIQUE \"</td>\n",
              "      <td>1409011200</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$8.40</td>\n",
              "      <td>p23649501</td>\n",
              "      <td>3856620</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a contemporary jazz and soul performer who's m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>05 11, 2003</td>\n",
              "      <td>u36200649</td>\n",
              "      <td>Very good idea to put both the 'pop' and 'orch...</td>\n",
              "      <td>Exceeded my Expectations - This album RAWKS!</td>\n",
              "      <td>1052611200</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$10.98</td>\n",
              "      <td>p58458313</td>\n",
              "      <td>56086781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>very good idea to put both the 'pop' and 'orch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>12 5, 2017</td>\n",
              "      <td>u10721702</td>\n",
              "      <td>This is a great collection of Carole King's so...</td>\n",
              "      <td>A Must-have for Carole King Fans</td>\n",
              "      <td>1512432000</td>\n",
              "      <td>Pop</td>\n",
              "      <td>$5.99</td>\n",
              "      <td>p97027626</td>\n",
              "      <td>55852154</td>\n",
              "      <td>NaN</td>\n",
              "      <td>this is a great collection of carole king's so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>09 24, 2013</td>\n",
              "      <td>u86003775</td>\n",
              "      <td>The is album is a brilliant piece of Jazz fusi...</td>\n",
              "      <td>A Master piece!</td>\n",
              "      <td>1379980800</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$14.64</td>\n",
              "      <td>p43167086</td>\n",
              "      <td>43228100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the is album is a brilliant piece of jazz fusi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>01 14, 2015</td>\n",
              "      <td>u25672859</td>\n",
              "      <td>Waited a LONG time for this DVD to be released...</td>\n",
              "      <td>especially if you like concert videos</td>\n",
              "      <td>1421193600</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$9.92</td>\n",
              "      <td>p94494236</td>\n",
              "      <td>54425467</td>\n",
              "      <td>NaN</td>\n",
              "      <td>waited a long time for this dvd to be released...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>5.0</td>\n",
              "      <td>09 12, 2013</td>\n",
              "      <td>u29136602</td>\n",
              "      <td>There is just something about Wesley's voice t...</td>\n",
              "      <td>Just right.</td>\n",
              "      <td>1378944000</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$9.49</td>\n",
              "      <td>p26326919</td>\n",
              "      <td>90575108</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is just something about wesley's voice t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>5.0</td>\n",
              "      <td>05 15, 2016</td>\n",
              "      <td>u51715193</td>\n",
              "      <td>How many recordings of Verdis Requiem do you o...</td>\n",
              "      <td>Being there  in vibrant immediacy of performan...</td>\n",
              "      <td>1463270400</td>\n",
              "      <td>Classical</td>\n",
              "      <td>$14.83</td>\n",
              "      <td>p69247882</td>\n",
              "      <td>74425587</td>\n",
              "      <td>NaN</td>\n",
              "      <td>how many recordings of verdis requiem do you o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>5.0</td>\n",
              "      <td>06 13, 2004</td>\n",
              "      <td>u28696060</td>\n",
              "      <td>....their best overall album.  After the revis...</td>\n",
              "      <td>Creative rebirth and possibly....</td>\n",
              "      <td>1087084800</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$18.86</td>\n",
              "      <td>p07028930</td>\n",
              "      <td>20399056</td>\n",
              "      <td>NaN</td>\n",
              "      <td>....their best overall album.  after the revis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>5.0</td>\n",
              "      <td>06 27, 2012</td>\n",
              "      <td>u04925906</td>\n",
              "      <td>Believe it or not, once upon a time, artists l...</td>\n",
              "      <td>Once Upon A Time</td>\n",
              "      <td>1340755200</td>\n",
              "      <td>Pop</td>\n",
              "      <td>$19.99</td>\n",
              "      <td>p38606785</td>\n",
              "      <td>5931588</td>\n",
              "      <td>NaN</td>\n",
              "      <td>believe it or not, once upon a time, artists l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>5.0</td>\n",
              "      <td>03 6, 2000</td>\n",
              "      <td>u79459462</td>\n",
              "      <td>MoKenStef's album is a must have for those who...</td>\n",
              "      <td>An intense album with creativity!</td>\n",
              "      <td>952300800</td>\n",
              "      <td>Pop</td>\n",
              "      <td>$9.99</td>\n",
              "      <td>p07495887</td>\n",
              "      <td>90305896</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mokenstef's album is a must have for those who...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows Ã— 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        overall  ...                                               text\n",
              "0           5.0  ...  a contemporary jazz and soul performer who's m...\n",
              "1           5.0  ...  very good idea to put both the 'pop' and 'orch...\n",
              "2           5.0  ...  this is a great collection of carole king's so...\n",
              "3           5.0  ...  the is album is a brilliant piece of jazz fusi...\n",
              "4           5.0  ...  waited a long time for this dvd to be released...\n",
              "...         ...  ...                                                ...\n",
              "149995      5.0  ...  there is just something about wesley's voice t...\n",
              "149996      5.0  ...  how many recordings of verdis requiem do you o...\n",
              "149997      5.0  ...  ....their best overall album.  after the revis...\n",
              "149998      5.0  ...  believe it or not, once upon a time, artists l...\n",
              "149999      5.0  ...  mokenstef's album is a must have for those who...\n",
              "\n",
              "[150000 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDgkAVjrijX_"
      },
      "source": [
        "df[\"text\"] = df[\"text\"].apply(remove_nonAlphaNum)         #remove commas and everythin\n",
        "df[\"text\"] = df[\"text\"].apply(removeWhiteSpace)           # remove line breaks and tabs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8LO7ZB2ijYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e825ea83-17d3-47bb-e84c-fd1f3f08c802"
      },
      "source": [
        "# 14 min 11 seconds under GPU and large RA; Google Colab\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop = stopwords.words('english')\n",
        "# Lemmatizing\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: lemmatize_sentence(x))\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   overall  ...                                               text\n",
            "0      5.0  ...  a contemporary jazz and soul performer who mak...\n",
            "1      5.0  ...  very good idea to put both the pop and orchest...\n",
            "2      5.0  ...  this be a great collection of carole king song...\n",
            "3      5.0  ...  the be album be a brilliant piece of jazz fusi...\n",
            "4      5.0  ...  wait a long time for this dvd to be release ha...\n",
            "\n",
            "[5 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDKoiNCaijYA"
      },
      "source": [
        "#remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "df[\"text\"]= df[\"text\"].apply(lambda x : ' '.join([word for word in x.split(' ') if word not in stop_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr6s1_MJijYA"
      },
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['text']= df['text'].apply(lambda x : ' '.join(st.stem(i) for i in x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpiRBrzbijYA"
      },
      "source": [
        "## remove single word character\n",
        "df[\"text\"]= df[\"text\"].apply(lambda x : ' '.join(i for i in x.split() if not (i.isalpha() and len(i)==1)))\n",
        "## remove numbers\n",
        "df[\"text\"]= df[\"text\"].apply(lambda x : re.sub(r'\\w*\\d+\\w*', '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-7ecTNBijYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e88c3ad-1dda-4465-a813-6050003c9b35"
      },
      "source": [
        "text = df[[\"text\"]]\n",
        "clean_train = df[[\"text\",\"overall\"]]\n",
        "clean_train.to_csv('clean_train.csv', sep = ',',index = False)\n",
        "print(clean_train.head())\n",
        "print(np.shape(clean_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  overall\n",
            "0  contemporari jazz soul perform make twodecad c...      5.0\n",
            "1  good idea put pop orchestr music matrix reload...      5.0\n",
            "2  great collect carol king songsa musthav carol ...      5.0\n",
            "3  album brilliant piec jazz fusion start finish ...      5.0\n",
            "4  wait long time dvd releas watch  yearold vh di...      5.0\n",
            "(150000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfE_e0rBijYB"
      },
      "source": [
        "### 1.3 Test Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHv6gBXnijYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b332307f-41d5-4533-aba6-855f9023194c"
      },
      "source": [
        "#read data \n",
        "df_test = pd.read_csv(\"test.csv\")       \n",
        "#combine summary and reviewText\n",
        "df_test[\"text\"] = df_test[\"reviewText\"] + df_test[\"summary\"] \n",
        "#lowercase\n",
        "df_test[\"text\"] = df_test[\"text\"].str.lower() \n",
        "#remove commas and everythin\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(remove_nonAlphaNum)\n",
        "# remove line breaks and tabs\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(removeWhiteSpace)           \n",
        "#lemmatize\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: lemmatize_sentence(x))\n",
        "#remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "df_test[\"text\"]= df_test[\"text\"].apply(lambda x : ' '.join([word for word in x.split(' ') if word not in stop_words]))\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df_test['text']= df_test['text'].apply(lambda x : ' '.join(st.stem(i) for i in x.split()))\n",
        "## remove single word character\n",
        "df_test[\"text\"]= df_test[\"text\"].apply(lambda x : ' '.join(i for i in x.split() if not (i.isalpha() and len(i)==1)))\n",
        "## remove numbers\n",
        "df_test[\"text\"]= df_test[\"text\"].apply(lambda x : re.sub(r'\\w*\\d+\\w*', '', x))\n",
        "\n",
        "\n",
        "# out put clean data\n",
        "clean_test = df_test[[\"text\"]]\n",
        "clean_test.to_csv('clean_test.csv', sep = ',',index = False)\n",
        "print(clean_test.head())\n",
        "print(np.shape(clean_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text\n",
            "0                                         okfiv star\n",
            "1    set alon worth purchas cost everyth els grav...\n",
            "2  mari blige someon peopl know first appear clas...\n",
            "3                          good satch albumfour star\n",
            "4  well hundr christma antholog one best ever mix...\n",
            "(20000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLrx_wLDijYC"
      },
      "source": [
        "### 1.4 WF, TFIDF, Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygyEe1RrijYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f8b4ab-e9af-4f50-b95e-b67fd0c206b4"
      },
      "source": [
        "#input file \n",
        "df = pd.read_csv('clean_train.csv', error_bad_lines = False)\n",
        "df.head()\n",
        "# check if null exists\n",
        "df['text'].isnull().values.any()\n",
        "for col in df.columns:\n",
        "    n_nan = df[col].isnull().sum()\n",
        "    if n_nan >0  :\n",
        "        print(n_nan, col)\n",
        "        \n",
        "#dropna\n",
        "# df_copy = df.dropna(axis = 0)\n",
        "\n",
        "#replace NaN with unknown \n",
        "#replace with unknown \n",
        "for col in df.columns:\n",
        "    df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "for col in df.columns:\n",
        "    n_nan = df[col].isnull().sum()\n",
        "    if n_nan >0  :\n",
        "        print(n_nan, col)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61 text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB6VdqrBijYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41abfa5-3719-4eef-830a-bf560b3c58bf"
      },
      "source": [
        "import copy\n",
        "df_copy = copy.deepcopy(df)\n",
        "df_copy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB7m77f6ijYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e0c95bef-cb82-49e5-a275-640fde25457c"
      },
      "source": [
        "df_copy = df_copy.rename(columns={'overall': 'true_rating'})\n",
        "df_copy.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>true_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>contemporari jazz soul perform make twodecad c...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good idea put pop orchestr music matrix reload...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great collect carol king songsa musthav carol ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>album brilliant piec jazz fusion start finish ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wait long time dvd releas watch  yearold vh di...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  true_rating\n",
              "0  contemporari jazz soul perform make twodecad c...          5.0\n",
              "1  good idea put pop orchestr music matrix reload...          5.0\n",
              "2  great collect carol king songsa musthav carol ...          5.0\n",
              "3  album brilliant piec jazz fusion start finish ...          5.0\n",
              "4  wait long time dvd releas watch  yearold vh di...          5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHHw9yYCfm4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "375185ee-cee4-4725-c1a9-0d49461062b2"
      },
      "source": [
        "#changes begin###############################\n",
        "#### train+test \n",
        "df_test = pd.read_csv('clean_test.csv', low_memory = False)\n",
        "df_test = df_test.rename(columns={'overall': 'true_rating'})\n",
        "for col in df_test.columns:\n",
        "    df_test[col] = df_test[col].fillna('Unknown')\n",
        "y = df_copy[['true_rating']]\n",
        "df_copy = df_copy.drop(['true_rating'], axis=1)\n",
        "df_copy "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>contemporari jazz soul perform make twodecad c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good idea put pop orchestr music matrix reload...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great collect carol king songsa musthav carol ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>album brilliant piec jazz fusion start finish ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wait long time dvd releas watch  yearold vh di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>someth wesley voic make swoon littl bit voic h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>mani record verdi requiem oftenask question se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>best overal album revisionist fallout follow g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>believ upon time artist like enchant could see...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>mokenstef album must wan na mellow chill favor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text\n",
              "0       contemporari jazz soul perform make twodecad c...\n",
              "1       good idea put pop orchestr music matrix reload...\n",
              "2       great collect carol king songsa musthav carol ...\n",
              "3       album brilliant piec jazz fusion start finish ...\n",
              "4       wait long time dvd releas watch  yearold vh di...\n",
              "...                                                   ...\n",
              "149995  someth wesley voic make swoon littl bit voic h...\n",
              "149996  mani record verdi requiem oftenask question se...\n",
              "149997  best overal album revisionist fallout follow g...\n",
              "149998  believ upon time artist like enchant could see...\n",
              "149999  mokenstef album must wan na mellow chill favor...\n",
              "\n",
              "[150000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukBum3zkfn4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9a0985aa-2c99-44f0-ec3d-b7289313b7b0"
      },
      "source": [
        "df_concat = pd.concat([df_copy, df_test])\n",
        "df_concat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>contemporari jazz soul perform make twodecad c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good idea put pop orchestr music matrix reload...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great collect carol king songsa musthav carol ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>album brilliant piec jazz fusion start finish ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wait long time dvd releas watch  yearold vh di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>good christian scott simpli take breath away b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>congratul clan xymox butcher  classic song wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>im happi cher video dvd couldnt rememb caus ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>agre comment make maxwel johnson regard sati b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>almost fair compar record rest ben harper bodi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>170000 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "0      contemporari jazz soul perform make twodecad c...\n",
              "1      good idea put pop orchestr music matrix reload...\n",
              "2      great collect carol king songsa musthav carol ...\n",
              "3      album brilliant piec jazz fusion start finish ...\n",
              "4      wait long time dvd releas watch  yearold vh di...\n",
              "...                                                  ...\n",
              "19995  good christian scott simpli take breath away b...\n",
              "19996  congratul clan xymox butcher  classic song wri...\n",
              "19997  im happi cher video dvd couldnt rememb caus ch...\n",
              "19998  agre comment make maxwel johnson regard sati b...\n",
              "19999  almost fair compar record rest ben harper bodi...\n",
              "\n",
              "[170000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAcvK57uijYD"
      },
      "source": [
        "#### 1.4.1 WF max_feature = 4000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puPLq3o1ijYD"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class WF_CV(object):\n",
        "    def __init__(self,dataFrame):\n",
        "        super().__init__()\n",
        "        self.dataFrame = dataFrame\n",
        "    \n",
        "    def convert(self, text_col):\n",
        "        return self.wf_convert(text_col)\n",
        "\n",
        "    def wf_convert(self, text_col):\n",
        "        wf_vectorizer = CountVectorizer(stop_words = 'english', max_features = 4000, max_df = 0.8)\n",
        "        X = wf_vectorizer.fit_transform(self.dataFrame[text_col])\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TikrNP7FijYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f37b477-5b4a-47ef-fc66-87540c73f9b7"
      },
      "source": [
        "\n",
        "WF_cvt = WF_CV(df_concat)\n",
        "X_concat_wf= WF_cvt.wf_convert('text')\n",
        "X_concat_wf "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<170000x4000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 6575228 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bm5sVJKfLBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb07ca3-1209-46d6-8d87-494773203610"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "X_concat_dense = csr_matrix.todense(X_concat_wf )\n",
        "X_concat_dense.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170000, 4000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxOqfBOfTVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d70eb2-dc30-435d-ac8f-c8e7ad351a9b"
      },
      "source": [
        "X_wf_dense = X_concat_dense[0:150000, :]\n",
        "X_test_test_wf_dense = X_concat_dense[150000: , :]\n",
        "\n",
        "X_wf_dense.shape, X_test_test_wf_dense.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150000, 4000), (20000, 4000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNoP0NQcl3Xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a7abbc-997a-4f81-9bf7-fa0e2890c78b"
      },
      "source": [
        "X_wf = csr_matrix(X_wf_dense)\n",
        "X_test_test_wf = csr_matrix(X_test_test_wf_dense)\n",
        "X_wf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<150000x4000 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 5815371 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg-8cxOIijYE"
      },
      "source": [
        "#WF data split\n",
        "X_train_wf, X_test_wf , y_train_wf, y_test_wf = train_test_split( X_wf, y, test_size = 0.3, random_state= 0, shuffle = False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NC-FyJdijYE"
      },
      "source": [
        "#### 1.4.2 TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyKI5jvdijYF"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "class TFIDF_Convert(object):\n",
        "    def __init__(self,dataFrame):\n",
        "        super().__init__()\n",
        "        self.dataFrame = dataFrame\n",
        "    \n",
        "    def convert(self, text_col):\n",
        "        return self.tfidf_convert(text_col)\n",
        "\n",
        "    def tfidf_convert(self, text_col):\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 15000, max_df = 0.8)\n",
        "        X = tfidf_vectorizer.fit_transform(self.dataFrame[text_col])\n",
        "        \n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AbKKXpTmHUh"
      },
      "source": [
        "TF_cvt = TFIDF_Convert(df_concat)\n",
        "X_concat_tf= TF_cvt.tfidf_convert('text')\n",
        "X_concat_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMLwWRRjmHYj"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "X_concat_dense_tf = csr_matrix.todense(X_concat_tf )\n",
        "X_concat_dense_tf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZR_g42WmHdg"
      },
      "source": [
        "X_tf_dense = X_concat_dense_tf[0:150000, :]\n",
        "X_test_test_tf_dense = X_concat_dense_tf[150000: , :]\n",
        "\n",
        "X_tf_dense.shape, X_test_test_tf_dense.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoyrK8MwmHhM"
      },
      "source": [
        "X_tf = csr_matrix(X_tf_dense)\n",
        "X_test_test_tf = csr_matrix(X_test_test_tf_dense)\n",
        "X_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJyQZ4T8ijYG"
      },
      "source": [
        "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split( X_tf, y, test_size = 0.3, random_state = 0, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2KBD-c4ijYG"
      },
      "source": [
        "##### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y4-HeEXijYG"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRuqmRhNijYG"
      },
      "source": [
        "#### 1.4.3 Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s85rUYQvijYG"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "class ngram_Convert(object):\n",
        "    def __init__(self,dataFrame):\n",
        "        super().__init__()\n",
        "        self.dataFrame = dataFrame\n",
        "    \n",
        "    def convert(self, text_col):\n",
        "        return self.ngram_convert(text_col)\n",
        "\n",
        "    def ngram_convert(self, text_col):\n",
        "        cv_ngram = CountVectorizer(max_features=4000,binary=True,ngram_range=(2, 2)) \n",
        "        X = cv_ngram.fit_transform(self.dataFrame[text_col])\n",
        "\n",
        "        return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9r5pG1YijYH"
      },
      "source": [
        "ngram_cvt = ngram_Convert(df_copy)\n",
        "X_ngram = ngram_cvt.convert('text')\n",
        "X_ngram "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUINF_kkijYH"
      },
      "source": [
        "X_train_ngram, X_test_ngram, y_train_ngram, y_test_ngram = train_test_split( X_ngram, y, test_size = 0.3, random_state = 0, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsDCTvyBijYH"
      },
      "source": [
        "## 2. Model Implementation\n",
        "We will use the following Models:\n",
        "\n",
        "\n",
        "1. Decision Tree\n",
        "\n",
        "2. Linear SVR\n",
        "\n",
        "3. Ridge Regression\n",
        "\n",
        "4. SGD Regressor\n",
        "\n",
        "5. Xgboost Regressor\n",
        "\n",
        "6. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYynDQdijYH"
      },
      "source": [
        "### 2.1 DT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJdgKzQoijYH"
      },
      "source": [
        "#### wf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOoLxJsjijYI"
      },
      "source": [
        "parameters={'min_samples_split': [10, 20, 40], \n",
        "                    'max_depth': [2, 8, 16], \n",
        "                    'min_samples_leaf': [20, 40, 100], \n",
        "                    'max_leaf_nodes': [5, 20, 100]}\n",
        "model =  DecisionTreeRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for DT is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbWBk1wkijYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cc0eaa-6375-4a1a-8bc4-3acf6ace11f7"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_wf_dt =DecisionTreeRegressor(min_samples_split=10 , max_depth=16, min_samples_leaf=40,max_leaf_nodes=100)\n",
        "model_opt_wf_dt.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_dt.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For WF feature\n",
            "MSE: 0.8235100005409548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzBEeQc6ijYI"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hmfQwFVijYI"
      },
      "source": [
        "parameters={'min_samples_split': [10, 20, 40], \n",
        "                    'max_depth': [2, 8, 16], \n",
        "                    'min_samples_leaf': [20, 40, 100], \n",
        "                    'max_leaf_nodes': [5, 20, 100]}\n",
        "model = DecisionTreeRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "print('For TFIDF features')\n",
        "print('The optimal parameters alpha for DT model is: \\n', clf.best_params_)\n",
        "# print('With cross-validation accuracy of: {}%'.format(round(clf.best_score_*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vXnSJmSijYI"
      },
      "source": [
        "model_opt_tf_dt = DecisionTreeRegressor(min_samples_split= 10, max_depth=16, min_samples_leaf=20,max_leaf_nodes=100)\n",
        "model_opt_tf_dt.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_dt.predict(X_test_tf)\n",
        "# acc_tf = accuracy_score(y_test_tf, y_pred_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "# print('Test accuracy of multinomial naive bayes model on WF is: {}%'.format(round(acc_tf *100, 2)))\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvPRbxXdijYI"
      },
      "source": [
        "#### Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJuYrlgzijYI"
      },
      "source": [
        "parameters={'min_samples_split': [10, 20, 40], \n",
        "                    'max_depth': [2, 8, 16], \n",
        "                    'min_samples_leaf': [20, 40, 100], \n",
        "                    'max_leaf_nodes': [5, 20, 100]}\n",
        "model = DecisionTreeRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('ngrams features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PL2jisNCEvg"
      },
      "source": [
        "model_opt_ngram_dt = DecisionTreeRegressor(min_samples_split= 40, max_depth=16, min_samples_leaf=20,max_leaf_nodes=100)\n",
        "model_opt_ngram_dt.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram_dt.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ya8MevAijYJ"
      },
      "source": [
        "### 2.2 LinearSVR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyLUkHMQrK8F"
      },
      "source": [
        "#### wf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPpuzUW4mb8O"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C': [0.001, 0.01,.01, 1.0, 10, 100],\n",
        "            'gammas' = [0.001, 0.01, 0.1, 1]}\n",
        "model = LinearSVR()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for Ridge model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J0clr7qEPzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc189461-79ad-422d-d5f1-037761a438c1"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_wf_svr =LinearSVR(C=0.01)\n",
        "model_opt_wf_svr.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_svr.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For WF feature\n",
            "MSE: 0.9011257181086527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4byh0J6orQ0A"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE_EpQC2EQzu"
      },
      "source": [
        "model = LinearSVR()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "print('For TFIDF features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)\n",
        "# print('With cross-validation accuracy of: {}%'.format(round(clf.best_score_*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty66DmnCESmR"
      },
      "source": [
        "model_opt_tf_svr = LinearSVR(C = 1.0)\n",
        "model_opt_tf_svr.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_svr.predict(X_test_tf)\n",
        "# acc_tf = accuracy_score(y_test_tf, y_pred_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "# print('Test accuracy of multinomial naive bayes model on WF is: {}%'.format(round(acc_tf *100, 2)))\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d7-Q2ysrC8U"
      },
      "source": [
        "#### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFO3aNjqEVwG"
      },
      "source": [
        "model = LinearSVR()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUdKDhdEV5W"
      },
      "source": [
        "model_opt_ngram_svr = LinearSVR(C=1.0)\n",
        "model_opt_ngram_svr.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram_svr.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UxAI2WhijYJ"
      },
      "source": [
        "### 2.3 Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YsKZVlm_9Up"
      },
      "source": [
        "#### wf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1wpLfsAS7T"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "parameters = {'alpha': [0.001, 0.01,.01, 1.0, 10, 100]}\n",
        "model = Ridge()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for Ridge model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kCCsPHDAS-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03fe71c-e1af-401e-816a-fb99b753e790"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_wf_ridge =Ridge(alpha=100)\n",
        "model_opt_wf_ridge.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_ridge.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For WF feature\n",
            "MSE: 0.7348134557885461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwxz-8a4ATB0"
      },
      "source": [
        "X_train_wf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "455vhnJ7__f5"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f28B1wkVAace"
      },
      "source": [
        "parameters = {'alpha': [0.001, 0.01,.01, 1.0, 10, 100]}\n",
        "model = Ridge()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "print('For TFIDF features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)\n",
        "# print('With cross-validation accuracy of: {}%'.format(round(clf.best_score_*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4veOI7wAagC"
      },
      "source": [
        "model_opt_tf_ridge = Ridge(alpha = 1.0)\n",
        "model_opt_tf_ridge.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_ridge.predict(X_test_tf)\n",
        "# acc_tf = accuracy_score(y_test_tf, y_pred_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "# print('Test accuracy of multinomial naive bayes model on WF is: {}%'.format(round(acc_tf *100, 2)))\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK2LsBzg__8c"
      },
      "source": [
        "#### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w9mcZlYAiWI"
      },
      "source": [
        "parameters = {'alpha': [0.001, 0.01,.01, 1.0, 10, 100]}\n",
        "model = Ridge()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('ngrams features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWSW4MX1Aih8"
      },
      "source": [
        "model_opt_ngram_ridge = Ridge(alpha=100)\n",
        "model_opt_ngram_ridge.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram_ridge.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3UtrQhnijYJ"
      },
      "source": [
        "### 2.4 SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZ85p0y_WGH"
      },
      "source": [
        "#### wf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5efQKRB_hS9"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "parameters = {'alpha': [0.001, 0.01,.01, 1.0, 10, 100]}\n",
        "model = SGDRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for Ridge model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AtM_pAm_hev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6108c864-1660-4207-b9a0-1fd2336d42b9"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "model_opt_wf_SGD =SGDRegressor(alpha=1.0)\n",
        "model_opt_wf_SGD.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_SGD.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For WF feature\n",
            "MSE: 0.9129196614198615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umbo348y_X2H"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNoLNRqr_pro"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_tf_SGD =SGDRegressor(alpha=1.0)\n",
        "model_opt_tf_SGD.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_SGD.predict(X_test_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "print('MSE:',mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl5OR-RM_put"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_tf_SGD = SGDRegressor(alpha = 0.001)\n",
        "model_opt_tf_SGD.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_SGD.predict(X_test_tf)\n",
        "# acc_tf = accuracy_score(y_test_tf, y_pred_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EPRUWvF_Ydm"
      },
      "source": [
        "#### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN59sgNq_w3B"
      },
      "source": [
        "parameters = {'alpha': [0.001, 0.01,.01, 1.0, 10, 100]}\n",
        "model = SGDRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('ngrams features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2saoqIU_w6Y"
      },
      "source": [
        "model_opt_ngram_SGD = SGDRegressor(alpha=0.001)\n",
        "model_opt_ngram_SGD.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram_SGD.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehNv-6bqijYJ"
      },
      "source": [
        "### 2.5 XG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4LNOHH08gTz"
      },
      "source": [
        "#### wf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU82XRW071Wn"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3lYqFdc8TPY"
      },
      "source": [
        "parameters = {'max_depth': [2,4,6,8,10]}\n",
        "model = XGBRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for XGBoost model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRKttrsk8TF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a232b0-89ab-4c2c-8478-126ee20c021b"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "model_opt_wf_XG =XGBRegressor(max_depth=6)\n",
        "model_opt_wf_XG.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_XG.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('Test MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[23:58:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "For WF feature\n",
            "Test MSE: 0.6949168924427146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZYNcoZ9GoM"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBGabfhH8p6D"
      },
      "source": [
        "parameters = {'max_depth': [2,4,6,8,10]}\n",
        "model = XGBRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "print('For TFIDF features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)\n",
        "# print('With cross-validation accuracy of: {}%'.format(round(clf.best_score_*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14yxk6GS9LR_"
      },
      "source": [
        "model_opt_tf_XG = XGBRegressor(max_depth = 6, gamma =  )\n",
        "model_opt_tf_XG.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_XG.predict(X_test_tf)\n",
        "\n",
        "\n",
        "print('For TFIDF feature')\n",
        "# print('Test accuracy of multinomial naive bayes model on WF is: {}%'.format(round(acc_tf *100, 2)))\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWQirc2v9DKJ"
      },
      "source": [
        "#### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4QJQQiq8p0Y"
      },
      "source": [
        "parameters = {'max_depth': [2,4,6,8,10]}\n",
        "model = XGBRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('ngrams features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Rr_SU98ptY"
      },
      "source": [
        "model_opt_ngram = XGBRegressor(max_depth = 8)\n",
        "model_opt_ngram.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMTZrdGqijYJ"
      },
      "source": [
        "### 2.6 RF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS6iR0DF-ESZ"
      },
      "source": [
        "#### wf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBBRzY0F-Mk-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "parameters = {'max_depth':[3,5,7,9],\n",
        "             'min_samples_split':[5,10,20]}\n",
        "model = RandomForestRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_wf, y_train_wf)\n",
        "\n",
        "print('WF features')\n",
        "print('The optimal parameters alpha for Ridge model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sifnGXdP-Muv"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "model_opt_wf_RF =RandomForestRegressor()\n",
        "model_opt_wf_RF.fit(X_train_wf, y_train_wf)\n",
        "y_pred_wf = model_opt_wf_RF.predict(X_test_wf)\n",
        "\n",
        "print('For WF feature')\n",
        "print('MSE:',mean_squared_error(y_test_wf, y_pred_wf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC-lQ5RB-E9-"
      },
      "source": [
        "#### tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMVaiH2Z-RTN"
      },
      "source": [
        "parameters = {'max_depth':[3,5,7,9],\n",
        "             'min_samples_split':[5,10,20]}\n",
        "model = RandomForestRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "print('For TFIDF features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)\n",
        "# print('With cross-validation accuracy of: {}%'.format(round(clf.best_score_*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T78QkUpB-RWH"
      },
      "source": [
        "model_opt_tf_RF = RandomForestRegressor( )\n",
        "model_opt_tf_RF.fit(X_train_tf, y_train_tf)\n",
        "y_pred_tf = model_opt_tf_RF.predict(X_test_tf)\n",
        "# acc_tf = accuracy_score(y_test_tf, y_pred_tf)\n",
        "\n",
        "print('For TFIDF feature')\n",
        "# print('Test accuracy of multinomial naive bayes model on WF is: {}%'.format(round(acc_tf *100, 2)))\n",
        "print('MSE:', mean_squared_error(y_test_tf, y_pred_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbdlJdaE-FYp"
      },
      "source": [
        "#### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A3L4JaA-Yw_"
      },
      "source": [
        "parameters = {'max_depth':[3,5,7,9],\n",
        "             'min_samples_split':[5,10,20]}\n",
        "model = RandomForestRegressor()\n",
        "clf = GridSearchCV(model, parameters, cv =10, n_jobs = -1)\n",
        "clf.fit(X_train_ngram, y_train_ngram)\n",
        "\n",
        "print('ngrams features')\n",
        "print('The optimal parameters alpha for multinomial naive bayes model is: \\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NquusuUS-Y2s"
      },
      "source": [
        "model_opt_ngram_RF = RandomForestRegressor(max_depth)\n",
        "model_opt_ngram_RF.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = model_opt_ngram_RF.predict(X_test_ngram)\n",
        "\n",
        "print('For N-gram feature')\n",
        "print('MSE:',mean_squared_error(y_test_ngram, y_pred_ngram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9wx1Y1vNoOo"
      },
      "source": [
        "# 3. Test Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3_wPQmVNu-a"
      },
      "source": [
        "### 3.1 Decision Tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em9ZEk9kO82-"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_dt = model_opt_wf_dt.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_dt = model_opt_tf_dt.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_dt = model_opt_ngram_dt.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klMACSCRmcvr"
      },
      "source": [
        "df_pairs_wf.to_csv('test_Keras_prediction_COMBINEDwf.csv', sep = ',', index = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Aw4qXfOFhW"
      },
      "source": [
        "### 3.2. Linear SVR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1TWl6rSgZZ"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_svr = model_opt_wf_svr.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_svr = model_opt_tf_svr.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_svr = model_opt_ngram_svr.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kd0V3iBOLk-"
      },
      "source": [
        "###3.3 Ridge Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sMnocmPO9rV"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_ridge = model_opt_wf_ridge.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_ridge = model_opt_tf_ridge.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_ridge = model_opt_ngram_ridge.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0vkvHc3OMRz"
      },
      "source": [
        "### 3.4 SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MgeMp9KbBIu"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_SGD = model_opt_wf_SGD.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_SGD = model_opt_tf_SGD.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_SGD = model_opt_ngram_SGD.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW71TL4TOMvm"
      },
      "source": [
        "### 3.5 XG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjJC8vVrbCm0"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_XG = model_opt_wf_XG.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_XG = model_opt_tf_XG.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_XG = model_opt_ngram_XG.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMM3xhbBONKX"
      },
      "source": [
        "### 3.6 RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBUGEYoGbDJB"
      },
      "source": [
        "# #wf\n",
        "y_pred_test_wf_RF = model_opt_wf_RF.predict(X_test_test_wf)\n",
        "\n",
        "#tfidf\n",
        "y_pred_test_tf_RF = model_opt_tf_RF.predict(X_test_test_tf)\n",
        "\n",
        "#ngram\n",
        "y_pred_test_ngram_RF = model_opt_ngram_RF.predict(X_test_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
